{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from gym.wrappers import Monitor\n",
    "from collections import deque, namedtuple\n",
    "from lib.plotting import EpisodeStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor:\n",
    "    TF_SCOPE_NAME = 'state_processor'\n",
    "    \n",
    "    def __init__(self):\n",
    "        with tf.variable_scope(StateProcessor.TF_SCOPE_NAME):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(self.output, [84, 84], tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "    \n",
    "    def process(self, session, state):\n",
    "        return session.run(self.output, {self.input_state: state})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator:\n",
    "    def __init__(self, valid_actions, scope='estimator', summaries_dir=None):\n",
    "        self.valid_actions = valid_actions\n",
    "        self.scope = scope\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, 'summaries_{}'.format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name='X')\n",
    "        self.Y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name='Y')\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name='actions')\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        conv1 = tf.contrib.layers.conv2d(X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, self.valid_actions.n)\n",
    "        \n",
    "        gather_indices = tf.range(batch_size) * self.valid_actions.n + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        self.losses = tf.squared_difference(self.Y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_optimization = self.optimizer.minimize(self.loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar('loss', self.loss),\n",
    "            tf.summary.histogram('loss_hist', self.losses),\n",
    "            tf.summary.histogram('q_values_hist', self.predictions),\n",
    "            tf.summary.scalar('max_q_value', tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "    def predict(self, session, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 84, 84]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return session.run(self.predictions, {self.X_pl: s})\n",
    "\n",
    "    def update(self, session, s, a, targets):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 84, 84]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = {self.X_pl: s, self.Y_pl: targets, self.actions_pl: a}\n",
    "        summaries, global_step, _, loss = session.run(\n",
    "            [self.summaries, tf.train.get_global_step(), self.train_optimization, self.loss],\n",
    "            feed_dict\n",
    "        )\n",
    "        \n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.00090302 0.         0.02340581]\n [0.         0.00090302 0.         0.02340581]]\n99.75719\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "e = Estimator(ENV.action_space, scope='test')\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    s = ENV.reset()\n",
    "    s_p = sp.process(sess, s)\n",
    "    s = np.stack([s_p] * 4, axis=2)\n",
    "    s = np.array([s] * 2)\n",
    "    \n",
    "    print(e.predict(sess, s))\n",
    "    \n",
    "    targets = np.array([10., 10.])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(sess, s, a, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model_parameters(session, source, target):\n",
    "    params_1 = [v for v in tf.trainable_variables() if v.name.startwith(source.scope)]\n",
    "    params_1 = sorted(params_1, key=lambda v: v.name)\n",
    "    params_2 = [v for v in tf.trainable_variables() if v.name.startwith(target.scope)]\n",
    "    params_2 = sorted(params_2, key=lambda v: v.name)\n",
    "    \n",
    "    update_operations = []\n",
    "    for v1, v2 in zip(params_1, params_2):\n",
    "        update_operations.append(v2.assign(v1))\n",
    "        \n",
    "    session.run(update_operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    def policy_function(session, s, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(session, np.expand_dims(s, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(session, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir,\n",
    "        replay_memory_size=500000, replay_memory_init_size=50000, update_target_estimator_every=10000,\n",
    "        discount_factor=0.99, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay_steps=500000, batch_size=32,\n",
    "        record_video_every=50):\n",
    "    Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "    replay_memory = []\n",
    "\n",
    "    stats = EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes)\n",
    "    )\n",
    "\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "    total_t = sess.run(tf.train.get_global_step())\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    policy = make_epsilon_greedy_policy(q_estimator, env.action_space.n)\n",
    "\n",
    "    print(\"Populating replay memory...\")\n",
    "    s = env.reset()\n",
    "    s = state_processor.process(sess, s)\n",
    "    s = np.stack([s] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        a_probs = policy(sess, s, epsilons[0])\n",
    "        a = np.random.choice(np.arange(len(a_probs)), p=a_probs)\n",
    "        s_prime, reward, done, _ = env.step(a)\n",
    "        s_prime = state_processor.process(sess, s_prime)\n",
    "        s_prime = np.stack([s, s, s, s_prime], axis=2)\n",
    "        replay_memory.append(Transition(state=s, action=a, reward=reward, next_state=s_prime, done=done))\n",
    "    \n",
    "    env = Monitor(env, directory=monitor_path, resume=True, \n",
    "                  video_callable=lambda count: count % record_video_every == 0)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "        \n",
    "        s = env.reset()\n",
    "        s = state_processor.process(sess, s)\n",
    "        s = np.stack([s] * 4, axis=2)\n",
    "        loss = None\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps - 1)]\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag='epsilon')\n",
    "            \n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(sess, q_estimator, target_estimator)\n",
    "                \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            a_probs = policy(sess, s, epsilon)\n",
    "            a = np.random.choice(np.arange(len(a_probs)), p=a_probs)\n",
    "            s_prime, reward, done, _ = env.step(a)\n",
    "            s_prime = state_processor.process(sess, s_prime)\n",
    "            s_prime = np.stack([s[1], s[2], s[3], s_prime], axis=2)\n",
    "            \n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "            replay_memory.append(Transition(state=s, action=a, reward=reward, next_state=s_prime, done=done))\n",
    "            \n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            transition_sample = np.random.choice(replay_memory, batch_size)\n",
    "            state_batch = np.array([t.state for t in transition_sample])\n",
    "            action_batch = np.array([t.action for t in transition_sample])\n",
    "            targets = target_estimator.predict(sess, state_batch)\n",
    "            \n",
    "            for i in range(len(targets)):\n",
    "                if transition_sample[i].done:\n",
    "                    targets[i] = reward\n",
    "                else:\n",
    "                    targets[i] = reward + discount_factor * targets[i]\n",
    "            \n",
    "            q_estimator.update(sess, state_batch, action_batch, targets)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            s = s_prime\n",
    "            total_t += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
